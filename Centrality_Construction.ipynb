{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "tvDKFTQnkxdC",
        "outputId": "0a85eb72-fc62-41aa-b4e3-5c5d8ea82068"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mrt.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3255883137.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf_stations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mrt.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf_od_am\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'od_pm_peak.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# df_od_am = pd.read_csv('od_pm_peak.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mrt.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from geopy.distance import geodesic\n",
        "import networkx as nx\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "df_stations = pd.read_csv('mrt.csv')\n",
        "df_od_am = pd.read_csv('od_pm_peak.csv')\n",
        "# df_od_am = pd.read_csv('od_pm_peak.csv')\n",
        "\n",
        "# Bersihkan ID\n",
        "df_stations['STN_NO'] = df_stations['STN_NO'].str.strip()\n",
        "df_od_am.columns = df_od_am.columns.str.strip()\n",
        "df_od_am['ORIGIN_PT_CODE'] = df_od_am['ORIGIN_PT_CODE'].str.strip()\n",
        "\n",
        "# Transform OD matrix ke long format\n",
        "df_od_long = df_od_am.melt(id_vars='ORIGIN_PT_CODE', var_name='destination', value_name='volume')\n",
        "df_od_long.rename(columns={'ORIGIN_PT_CODE': 'origin'}, inplace=True)\n",
        "df_od_long = df_od_long[df_od_long['volume'] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "# Buat kolom berisi daftar sub-ID\n",
        "df_stations['sub_ids'] = df_stations['STN_NO'].apply(lambda x: [s.strip() for s in x.split('/')])\n",
        "\n",
        "# Tambahkan node per sub-ID\n",
        "for _, row in df_stations.iterrows():\n",
        "    for sid in row['sub_ids']:\n",
        "        G.add_node(sid, lat=row['Latitude'], lon=row['Longitude'], line=row['COLOR'])\n",
        "\n",
        "# Tambahkan edge antar sub-ID jika interchange\n",
        "for _, row in df_stations.iterrows():\n",
        "    if len(row['sub_ids']) > 1:\n",
        "        subs = row['sub_ids']\n",
        "        for i in range(len(subs)):\n",
        "            for j in range(i + 1, len(subs)):\n",
        "                G.add_edge(subs[i], subs[j], weight=0.01)  # Virtual interchange connection"
      ],
      "metadata": {
        "id": "i2XeyF7b8CI8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabungkan koordinat\n",
        "station_coords = df_stations.set_index('STN_NO')[['Latitude', 'Longitude']].to_dict('index')\n",
        "\n",
        "def calc_distance(row):\n",
        "    o = station_coords.get(row['origin'])\n",
        "    d = station_coords.get(row['destination'])\n",
        "    if o and d:\n",
        "        return geodesic((o['Latitude'], o['Longitude']), (d['Latitude'], d['Longitude'])).km\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "df_od_long['distance_km'] = df_od_long.apply(calc_distance, axis=1)\n",
        "df_od_long = df_od_long.dropna()"
      ],
      "metadata": {
        "id": "LPJaLseek9tZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold: volume tinggi dan jarak jauh\n",
        "threshold_vol = df_od_long['volume'].quantile(0.90)\n",
        "threshold_dist = 8  # km\n",
        "\n",
        "df_candidates = df_od_long[\n",
        "    (df_od_long['volume'] > threshold_vol) &\n",
        "    (df_od_long['distance_km'] > threshold_dist)\n",
        "].copy()"
      ],
      "metadata": {
        "id": "7Xo0Wu936gP3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "# # Tambahkan node\n",
        "for _, row in df_stations.iterrows():\n",
        "    G.add_node(row['STN_NO'], lat=row['Latitude'], lon=row['Longitude'], line=row['COLOR'])\n",
        "\n",
        "# Tambahkan edge jika ada field adjacency (jika tidak, ini bisa dikustom manual)\n",
        "# Contoh: tambahkan edge antar stasiun berdasarkan urutan nama jalur\n",
        "# df_stations['sub_ids'] = df_stations['STN_NO'].apply(lambda x: [s.strip() for s in x.split('/')])\n",
        "lines = df_stations['COLOR'].unique()\n",
        "\n",
        "# G = nx.Graph()\n",
        "\n",
        "# Tambahkan semua sub-ID sebagai node\n",
        "for _, row in df_stations.iterrows():\n",
        "    for sid in row['sub_ids']:\n",
        "        G.add_node(sid, lat=row['Latitude'], lon=row['Longitude'], line=row['COLOR'])\n",
        "\n",
        "# Tambahkan edge antar sub-id jika satu stasiun memiliki lebih dari satu ID (interchange)\n",
        "for _, row in df_stations.iterrows():\n",
        "    sub_ids = row['sub_ids']\n",
        "    if len(sub_ids) > 1:\n",
        "        for i in range(len(sub_ids)):\n",
        "            for j in range(i+1, len(sub_ids)):\n",
        "                G.add_edge(sub_ids[i], sub_ids[j], weight=0.01)  # edge virtual interchange\n",
        "\n",
        "for line in df_stations['COLOR'].unique():\n",
        "    subset = df_stations[df_stations['COLOR'] == line].sort_values(by='STN_NAME')\n",
        "    station_lists = subset['sub_ids'].tolist()\n",
        "    for i in range(len(station_lists)-1):\n",
        "        for a in station_lists[i]:\n",
        "            for b in station_lists[i+1]:\n",
        "                G.add_edge(a, b, weight=1.0)\n",
        "\n",
        "centrality = nx.betweenness_centrality(G, weight='weight')\n",
        "closeness = nx.closeness_centrality(G, distance='weight')\n",
        "nx.set_node_attributes(G, centrality, 'betweenness')\n",
        "nx.set_node_attributes(G, closeness, 'closeness')\n",
        "\n",
        "df_centrality = pd.DataFrame({\n",
        "    'station_id': list(centrality.keys()),\n",
        "    'betweenness': list(centrality.values()),\n",
        "    'closeness': [closeness[k] for k in centrality.keys()]\n",
        "})\n",
        "\n",
        "# Top 5\n",
        "top_betweenness = df_centrality.sort_values(by='betweenness', ascending=False).head(5)\n",
        "top_closeness = df_centrality.sort_values(by='closeness', ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Betweenness Centrality:\")\n",
        "print(top_betweenness)\n",
        "\n",
        "print(\"\\nTop 5 Closeness Centrality:\")\n",
        "print(top_closeness)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf3ijoeP6reX",
        "outputId": "3008e168-f479-480d-d9f0-c04e87ba06fa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Betweenness Centrality:\n",
            "   station_id  betweenness  closeness\n",
            "91        EW6     0.017277   0.021984\n",
            "80       EW26     0.017277   0.021984\n",
            "65       EW11     0.017195   0.021857\n",
            "64       EW10     0.017195   0.021857\n",
            "63        EW1     0.016454   0.021129\n",
            "\n",
            "Top 5 Closeness Centrality:\n",
            "    station_id  betweenness  closeness\n",
            "99        NE13     0.003702   0.023674\n",
            "207        NE7     0.001645   0.023670\n",
            "208       DT12     0.001645   0.023670\n",
            "216       NS27     0.004319   0.023270\n",
            "217        CE2     0.004319   0.023270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# Memuat data dari file CSV\n",
        "df = pd.read_csv('od_pm_peak.csv')\n",
        "\n",
        "# Menampilkan 5 baris pertama dan informasi kolom untuk memahami struktur data\n",
        "print(\"5 baris pertama dari dataframe:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nInformasi dataframe:\")\n",
        "print(df.info())\n",
        "\n",
        "# Membuat Directed Graph (graf berarah)\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Mengumpulkan nama-nama node asal (origin) dari kolom 'ORIGIN_PT_CODE'\n",
        "origin_nodes = df['ORIGIN_PT_CODE'].tolist()\n",
        "\n",
        "# Mengumpulkan nama-nama node tujuan (destination) dari semua kolom lainnya\n",
        "destination_columns = df.columns[1:]\n",
        "\n",
        "# Menambahkan semua node ke graf\n",
        "G.add_nodes_from(origin_nodes)\n",
        "G.add_nodes_from(destination_columns)\n",
        "\n",
        "# Menambahkan edge (tepi/sisi) ke graf dengan bobot\n",
        "# Setiap sel (origin, destination) dengan nilai > 0 dianggap sebagai edge\n",
        "for index, row in df.iterrows():\n",
        "    origin = row['ORIGIN_PT_CODE']\n",
        "    for destination_col in destination_columns:\n",
        "        weight = row[destination_col]\n",
        "        if weight > 0:\n",
        "            G.add_edge(origin, destination_col, weight=weight)\n",
        "\n",
        "# Menghitung Betweenness Centrality\n",
        "# Bobot edge digunakan dalam perhitungan. Edge dengan bobot lebih tinggi akan memiliki dampak lebih besar.\n",
        "betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
        "\n",
        "# Menghitung Closeness Centrality\n",
        "# Closeness centrality dihitung pada komponen yang sangat terhubung (strongly connected component).\n",
        "# Jika graf tidak sangat terhubung, maka akan dihitung pada komponen sangat terhubung terbesar.\n",
        "if not nx.is_strongly_connected(G):\n",
        "    print(\"\\nGraf tidak sangat terhubung. Menghitung closeness centrality pada komponen sangat terhubung terbesar.\")\n",
        "    scc = list(nx.strongly_connected_components(G))\n",
        "    if scc:\n",
        "        largest_scc = G.subgraph(max(scc, key=len))\n",
        "        closeness_centrality = nx.closeness_centrality(largest_scc, distance='weight')\n",
        "    else:\n",
        "        closeness_centrality = {} # Tidak ada SCC yang ditemukan\n",
        "else:\n",
        "    closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
        "\n",
        "# Mengubah hasil ke DataFrame untuk tampilan yang lebih baik\n",
        "betweenness_df = pd.DataFrame(betweenness_centrality.items(), columns=['Node', 'Betweenness Centrality'])\n",
        "closeness_df = pd.DataFrame(closeness_centrality.items(), columns=['Node', 'Closeness Centrality'])\n",
        "\n",
        "# Mengurutkan hasil dari yang tertinggi ke terendah\n",
        "betweenness_df = betweenness_df.sort_values(by='Betweenness Centrality', ascending=False)\n",
        "closeness_df = closeness_df.sort_values(by='Closeness Centrality', ascending=False)\n",
        "\n",
        "# Menampilkan 10 node teratas berdasarkan Betweenness Centrality\n",
        "print(\"\\n10 Node Teratas Berdasarkan Betweenness Centrality:\")\n",
        "print(betweenness_df.head(5))\n",
        "\n",
        "# Menampilkan 10 node teratas berdasarkan Closeness Centrality\n",
        "print(\"\\n10 Node Teratas Berdasarkan Closeness Centrality:\")\n",
        "print(closeness_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsJIBKr6Ol7K",
        "outputId": "f6996d7e-3894-4fee-9a8c-dd94afca6741"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 baris pertama dari dataframe:\n",
            "  ORIGIN_PT_CODE      BP10      BP11      BP12      BP13       BP2       BP3  \\\n",
            "0           BP10  0.000000  0.049985  0.046155  0.030837  0.026806  0.028016   \n",
            "1           BP11  0.035133  0.000000  0.026646  0.024771  0.022205  0.023882   \n",
            "2           BP12  0.044909  0.030683  0.000000  0.020177  0.027243  0.031427   \n",
            "3           BP13  0.061707  0.054491  0.028116  0.000000  0.030356  0.032346   \n",
            "4            BP2  0.032347  0.024635  0.020951  0.012202  0.000000  0.038103   \n",
            "\n",
            "        BP4       BP5   BP6/DT1  ...      TE26      TE27      TE28      TE29  \\\n",
            "0  0.013907  0.038194  0.331150  ...  0.000000  0.000000  0.000000  0.000101   \n",
            "1  0.022797  0.063061  0.375308  ...  0.000099  0.000000  0.000000  0.000000   \n",
            "2  0.024826  0.044258  0.400186  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "3  0.032595  0.019905  0.276935  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "4  0.019339  0.039599  0.270749  ...  0.000230  0.000115  0.000115  0.000000   \n",
            "\n",
            "        TE3       TE4       TE5       TE6       TE7       TE8  \n",
            "0  0.000605  0.000000  0.000202  0.000101  0.000000  0.000101  \n",
            "1  0.000789  0.000099  0.000000  0.000099  0.000099  0.000000  \n",
            "2  0.001116  0.000372  0.000279  0.000093  0.000000  0.000093  \n",
            "3  0.000498  0.000000  0.000000  0.000498  0.000249  0.000000  \n",
            "4  0.004029  0.000460  0.000115  0.001036  0.000460  0.000230  \n",
            "\n",
            "[5 rows x 183 columns]\n",
            "\n",
            "Informasi dataframe:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 182 entries, 0 to 181\n",
            "Columns: 183 entries, ORIGIN_PT_CODE to TE8\n",
            "dtypes: float64(182), object(1)\n",
            "memory usage: 260.3+ KB\n",
            "None\n",
            "\n",
            "10 Node Teratas Berdasarkan Betweenness Centrality:\n",
            "              Node  Betweenness Centrality\n",
            "39            DT17                0.223634\n",
            "83            EW30                0.179926\n",
            "84            EW31                0.155862\n",
            "6              BP4                0.075107\n",
            "124  NS27/CE2/TE20                0.063137\n",
            "\n",
            "10 Node Teratas Berdasarkan Closeness Centrality:\n",
            "     Node  Closeness Centrality\n",
            "83   EW30          11064.718005\n",
            "9     BP7          11024.550423\n",
            "139   PW1          10981.873452\n",
            "150   SE5          10904.938050\n",
            "84   EW31          10829.229148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inflow = df_od_long.groupby('destination')['volume'].sum()\n",
        "outflow = df_od_long.groupby('origin')['volume'].sum()\n",
        "\n",
        "df_stations['inflow'] = df_stations['STN_NO'].map(inflow).fillna(0)\n",
        "df_stations['outflow'] = df_stations['STN_NO'].map(outflow).fillna(0)\n",
        "df_stations['betweenness'] = df_stations['STN_NO'].map(centrality).fillna(0)\n",
        "df_stations['closeness'] = df_stations['STN_NO'].map(closeness).fillna(0)"
      ],
      "metadata": {
        "id": "jQR3Dv_K60VD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_stations[['Latitude', 'Longitude', 'inflow', 'outflow', 'betweenness', 'closeness']]\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "df_stations['cluster'] = kmeans.fit_predict(X_scaled)"
      ],
      "metadata": {
        "id": "HSdZnD_P63cP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_map = df_stations.set_index('STN_NO')['cluster'].to_dict()\n",
        "df_od_long['cluster_o'] = df_od_long['origin'].map(cluster_map)\n",
        "df_od_long['cluster_d'] = df_od_long['destination'].map(cluster_map)\n",
        "\n",
        "df_interzone = df_od_long[df_od_long['cluster_o'] != df_od_long['cluster_d']]\n",
        "df_corridors = df_interzone.groupby(['cluster_o', 'cluster_d'])['volume'].sum().reset_index()"
      ],
      "metadata": {
        "id": "xE_tDzRz66kw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Muat dataset\n",
        "df = pd.read_csv('od_am_peak.csv')\n",
        "\n",
        "# Simpan ORIGIN_PT_CODE secara terpisah sebelum menghapus untuk pengelompokan\n",
        "origin_codes = df['ORIGIN_PT_CODE']\n",
        "\n",
        "# Hapus kolom 'ORIGIN_PT_CODE' karena bukan fitur numerik untuk pengelompokan\n",
        "X = df.drop('ORIGIN_PT_CODE', axis=1)\n",
        "\n",
        "# Standarisasi data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Tentukan jumlah klaster optimal (K) menggunakan Metode Elbow\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Berdasarkan plot metode elbow dari eksekusi sebelumnya, K=4 dipilih sebagai optimal.\n",
        "# 3. Terapkan pengelompokan K-Means dengan K=4\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Tambahkan label klaster ke DataFrame asli\n",
        "df['Cluster'] = cluster_labels\n",
        "\n",
        "# 4. Tampilkan data yang dikelompokkan\n",
        "clustered_stations = df[['ORIGIN_PT_CODE', 'Cluster']]\n",
        "print(\"\\nStasiun yang Dikaster (5 baris pertama):\")\n",
        "print(clustered_stations.head())\n",
        "\n",
        "# Simpan stasiun yang dikelompokkan ke file CSV\n",
        "clustered_stations.to_csv('clustered_stations.csv', index=False)\n",
        "\n",
        "# Analisis karakteristik setiap klaster untuk rekomendasi rute\n",
        "# Kelompokkan berdasarkan klaster dan hitung rata-rata fitur numerik (titik tujuan)\n",
        "# Kita perlu mengecualikan 'ORIGIN_PT_CODE' dari perhitungan rata-rata ini, karena itu adalah tipe objek.\n",
        "# Kita akan menggunakan X asli (fitur numerik) dan menambahkan label klaster ke dalamnya.\n",
        "X_clustered = X.copy()\n",
        "X_clustered['Cluster'] = cluster_labels\n",
        "cluster_centers = X_clustered.groupby('Cluster').mean()\n",
        "\n",
        "num_top_destinations = 5\n",
        "route_recommendations = {}\n",
        "\n",
        "for cluster_id in sorted(df['Cluster'].unique()):\n",
        "    # Dapatkan tujuan (kolom tidak termasuk 'Cluster' dari cluster_centers)\n",
        "    cluster_data = cluster_centers.loc[cluster_id]\n",
        "    top_destinations = cluster_data.nlargest(num_top_destinations)\n",
        "    route_recommendations[f'Cluster {cluster_id}'] = top_destinations.index.tolist()\n",
        "\n",
        "print(\"\\nRekomendasi Rute per Klaster:\")\n",
        "for cluster, destinations in route_recommendations.items():\n",
        "    print(f\"{cluster}: {', '.join(destinations)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOHjlCn469eY",
        "outputId": "fcf0fd08-0cf5-4c78-9ea6-304413c56df3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stasiun yang Dikaster (5 baris pertama):\n",
            "  ORIGIN_PT_CODE  Cluster\n",
            "0           BP10        2\n",
            "1           BP11        2\n",
            "2           BP12        2\n",
            "3           BP13        2\n",
            "4            BP2        2\n",
            "\n",
            "Rekomendasi Rute per Klaster:\n",
            "Cluster 0: NE1/CC29, EW14/NS26, DT17, CC24, NE17/PTC\n",
            "Cluster 1: DT17, DT18, CG1/DT35, NE4/DT19, EW2/DT32\n",
            "Cluster 2: EW14/NS26, BP6/DT1, EW24/NS1, EW15, EW27\n",
            "Cluster 3: TE19, NS21/DT11, EW14/NS26, DT17, TE14/NS22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Muat dataset\n",
        "df = pd.read_csv('od_pm_peak.csv')\n",
        "\n",
        "# Simpan 'ORIGIN_PT_CODE' sebelum dihapus dari data yang akan di-cluster\n",
        "origin_pt_codes = df['ORIGIN_PT_CODE']\n",
        "\n",
        "# Drop kolom 'ORIGIN_PT_CODE' karena bukan fitur numerik untuk clustering\n",
        "X = df.drop('ORIGIN_PT_CODE', axis=1)\n",
        "\n",
        "# Standarisasi data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Terapkan K-Means clustering (menggunakan K=4 seperti yang ditentukan sebelumnya dari metode elbow)\n",
        "# Karena sudah ada hasil plot elbow method sebelumnya, kita langsung pakai K=4\n",
        "kmeans = KMeans(n_clusters=7, init='k-means++', random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Tambahkan label klaster ke DataFrame asli\n",
        "df['Cluster'] = cluster_labels\n",
        "\n",
        "# 3. Format data klaster agar sesuai dengan gambar\n",
        "# Buat list untuk menyimpan data yang diformat\n",
        "clustered_data_list = []\n",
        "\n",
        "# Iterasi melalui setiap klaster\n",
        "for cluster_id in sorted(df['Cluster'].unique()):\n",
        "    # Dapatkan semua ORIGIN_PT_CODE untuk klaster saat ini\n",
        "    stations_in_cluster = df[df['Cluster'] == cluster_id]['ORIGIN_PT_CODE'].tolist()\n",
        "    # Gabungkan nama stasiun dengan koma\n",
        "    stations_str = ','.join(stations_in_cluster)\n",
        "    # Tambahkan ke list dengan format {Cluster ID: ..., Stations: ...}\n",
        "    clustered_data_list.append({'Cluster ID': cluster_id, 'Stations': stations_str})\n",
        "\n",
        "# Buat DataFrame dari list yang diformat\n",
        "clustered_stations_df = pd.DataFrame(clustered_data_list)\n",
        "\n",
        "# 4. Tampilkan DataFrame hasil pengelompokan\n",
        "print(\"Data Stasiun yang Dikaster (K-Means):\")\n",
        "print(clustered_stations_df.to_string(index=False)) # Menggunakan to_string(index=False) untuk mencetak tanpa indeks DataFrame Pandas\n",
        "\n",
        "# Anda juga bisa menyimpannya ke CSV jika diperlukan\n",
        "# clustered_stations_df.to_csv('clustered_stations_kmeans_formatted.csv', index=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9KRTVKQanwT",
        "outputId": "8585238b-9cca-47e9-bf51-12654287fa4f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Stasiun yang Dikaster (K-Means):\n",
            " Cluster ID                                                                                                                                                                                                                                                                                                                                            Stations\n",
            "          0 CC10/DT26,CC11,CC12,CC14,CC16,CC2,CC20,CC21,CC23,CC24,CC25,CC26,CC27,CC28,CC3,CC5,CC6,CC7,CC8,CG2,EW1,EW10,EW11,EW12/DT14,EW14/NS26,EW15,EW16/NE3/TE17,EW21/CC22,EW3,EW4,EW5,EW6,EW7,EW8/CC9,EW9,NE1/CC29,NE4/DT19,NS10,NS11,NS12,NS13,NS14,NS15,NS16,NS17/CC15,NS18,NS19,NS20,NS23,NS24/NE6/CC1,NS25/EW13,NS27/CE2/TE20,NS28,NS8,NS9/TE2,TE14/NS22\n",
            "          1                                                                                                                                                          NE10,NE11,NE12/CC13,NE13,NE14,NE15,NE16/STC,NE17/PTC,NE18,NE5,NE7/DT12,NE8,NE9,PE1,PE2,PE3,PE4,PE5,PE6,PE7,PW1,PW2,PW3,PW4,PW5,PW6,PW7,SE1,SE2,SE3,SE4,SE5,SW1,SW2,SW3,SW4,SW5,SW6,SW7,SW8\n",
            "          2                                                                                                                                                                                                                                         EW17,EW18,EW19,EW20,EW22,EW23,EW24/NS1,EW25,EW26,EW27,EW28,EW29,EW30,EW31,EW32,EW33,NS2,NS3,NS4/BP1,NS5,NS7\n",
            "          3                                                                                                                                                                                                                                                                  CG1/DT35,DT20,DT21,DT22,DT23,DT24,DT25,DT27,DT28,DT29,DT30,DT31,DT33,DT34,EW2/DT32\n",
            "          4                                                                                                                                                                                                                                     CC17/TE9,TE1,TE11,TE12,TE13,TE15,TE16,TE18,TE19,TE22,TE23,TE24,TE25,TE26,TE27,TE28,TE29,TE3,TE4,TE5,TE6,TE7,TE8\n",
            "          5                                                                                                                                                                                                                                                                                                     BP10,BP11,BP12,BP13,BP2,BP3,BP4,BP5,BP7,BP8,BP9\n",
            "          6                                                                                                                                                                                                                                                        BP6/DT1,CC19/DT9,CC4/DT15,CE1/DT16,DT10,DT13,DT17,DT18,DT2,DT3,DT4,DT5,DT6,DT7,DT8,NS21/DT11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "# Load data\n",
        "df_station = pd.read_csv('mrt.csv')\n",
        "df_od_matrix = pd.read_csv('od_am_peak.csv')\n",
        "\n",
        "# Bersihkan data\n",
        "df_station['STN_NO'] = df_station['STN_NO'].str.strip()\n",
        "df_od_matrix.columns = df_od_matrix.columns.str.strip()\n",
        "\n",
        "# Buat posisi stasiun\n",
        "station_pos = {\n",
        "    row['STN_NO']: (row['Latitude'], row['Longitude'])\n",
        "    for _, row in df_station.iterrows()\n",
        "}\n",
        "\n",
        "# Transform OD matrix ke long format\n",
        "df_od_long = df_od_matrix.melt(id_vars='ORIGIN_PT_CODE', var_name='DESTINATION_PT_CODE', value_name='volume')\n",
        "df_od_long.columns = ['origin', 'destination', 'volume']\n",
        "df_od_long = df_od_long[df_od_long['volume'].astype(float) > 0]\n",
        "\n",
        "# Hitung jarak\n",
        "def compute_distance(row):\n",
        "    try:\n",
        "        return geodesic(station_pos[row['origin']], station_pos[row['destination']]).km\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df_od_long['distance_km'] = df_od_long.apply(compute_distance, axis=1)\n",
        "df_od_long.dropna(subset=['distance_km'], inplace=True)\n",
        "\n",
        "# Threshold kandidat ekspres\n",
        "volume_threshold = df_od_long['volume'].astype(float).quantile(0.95)\n",
        "distance_threshold = df_od_long['distance_km'].quantile(0.75)\n",
        "\n",
        "# Filter\n",
        "df_express = df_od_long[\n",
        "    (df_od_long['volume'].astype(float) >= volume_threshold) &\n",
        "    (df_od_long['distance_km'] >= distance_threshold)\n",
        "]\n",
        "\n",
        "# Tampilkan kandidat jalur ekspres\n",
        "print(df_express.sort_values(by='volume', ascending=False).head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L37Au_i3AKXO",
        "outputId": "6344fae7-6b18-4e82-e1cb-98930ee75945"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         origin destination    volume  distance_km\n",
            "12073       EW1        EW15  0.067250    15.688308\n",
            "7106    BP6/DT1        DT17  0.066644    14.979302\n",
            "11940      NS12   EW14/NS26  0.063671    17.744554\n",
            "11941      NS13   EW14/NS26  0.054048    16.172449\n",
            "11908      EW26   EW14/NS26  0.052564    15.970833\n",
            "11942      NS14   EW14/NS26  0.051001    14.877752\n",
            "23562      EW31         NS7  0.045901    17.057242\n",
            "6677       NS28         CG2  0.045652    16.892632\n",
            "1490   CE1/DT16     BP6/DT1  0.044259    15.268505\n",
            "11684       CG2   EW12/DT14  0.044133    16.042205\n"
          ]
        }
      ]
    }
  ]
}